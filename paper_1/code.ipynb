{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa7c11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.layers import Embedding, Dot, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcdccf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 准备数据\n",
    "corpus = [\n",
    "    \"tensorflow is a deep learning framework\",\n",
    "    \"word embeddings are useful in many NLP tasks\",\n",
    "    \"skip gram is a word embedding technique\",\n",
    "    \"efficient estimation of word representations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb03e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 10.6278\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 899us/step - loss: 10.1168\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.9445\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.8045\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 9.3313\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 957us/step - loss: 8.9667\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.5839\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 8.2843\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 956us/step - loss: 8.1224\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 961us/step - loss: 8.0064\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 文本预处理 - 将文本转换为数字序列\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)  # 根据语料库进行词汇映射\n",
    "word2id = tokenizer.word_index  # 词到ID的映射\n",
    "id2word = {v: k for k, v in word2id.items()}  # ID到词的映射\n",
    "sequences = tokenizer.texts_to_sequences(corpus)  # 生成序列化后的句子\n",
    "\n",
    "# Step 3: 定义模型的超参数\n",
    "vocab_size = len(word2id) + 1  # 词汇表的大小\n",
    "window_size = 2  # Skip-Gram窗口大小\n",
    "embedding_dim = 100  # 词向量的维度\n",
    "\n",
    "# Step 4: 生成 Skip-Gram 训练数据\n",
    "def generate_training_data(sequences, window_size, vocab_size):\n",
    "    all_pairs = []\n",
    "    for tokens in sequences:\n",
    "        pairs, _ = skipgrams(tokens, vocab_size, window_size=window_size)  # 生成Skip-Gram数据\n",
    "        all_pairs.extend(pairs)\n",
    "    return np.array(all_pairs)\n",
    "\n",
    "pairs = generate_training_data(sequences, window_size, vocab_size)\n",
    "targets, contexts = pairs[:, 0], pairs[:, 1]  # 提取目标词和上下文词\n",
    "\n",
    "# Step 5: 创建 TensorFlow 数据集\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices((targets, contexts))  # 构建TensorFlow数据集\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)  # 打乱并分批数据\n",
    "\n",
    "# Step 6: 构建 Skip-Gram 模型\n",
    "target_input = tf.keras.layers.Input(shape=(), dtype='int32')  # 目标词输入\n",
    "context_input = tf.keras.layers.Input(shape=(), dtype='int32')  # 上下文词输入\n",
    "\n",
    "embedding = Embedding(vocab_size, embedding_dim, name='embedding')  # 嵌入层，显式指定名称\n",
    "\n",
    "target_embedding = embedding(target_input)  # 获取目标词嵌入向量\n",
    "context_embedding = embedding(context_input)  # 获取上下文词嵌入向量\n",
    "\n",
    "dot_product = Dot(axes=-1)([target_embedding, context_embedding])  # 计算点积（相似度分数）\n",
    "\n",
    "output = Reshape((1,))(dot_product)  # 调整输出形状\n",
    "\n",
    "skipgram_model = Model([target_input, context_input], output)  # 构建模型\n",
    "skipgram_model.compile(loss='binary_crossentropy', optimizer=Adam())  # 编译模型\n",
    "\n",
    "# Step 7: 训练模型\n",
    "labels = np.ones(len(targets))  # 所有正样本标签为1\n",
    "skipgram_model.fit([targets, contexts], labels, epochs=10, batch_size=BATCH_SIZE)  # 开始训练模型\n",
    "\n",
    "# Step 8: 提取训练好的词嵌入\n",
    "word_embeddings = skipgram_model.get_layer('embedding').get_weights()[0]  # 提取词向量\n",
    "\n",
    "# Step 9: 查找相似词\n",
    "def get_similar_words(word, word_embeddings, word2id, id2word, top_n=5):\n",
    "    word_vec = word_embeddings[word2id[word]]  # 获取指定词的词向量\n",
    "    sim_scores = np.dot(word_embeddings, word_vec)  # 计算余弦相似度\n",
    "    sim_ids = np.argsort(sim_scores)[-top_n-1:-1][::-1]  # 找出最相似的词\n",
    "    return [id2word[i] for i in sim_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a3c09c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'estimation': ['of', 'efficient', 'word', 'framework', 'are']\n"
     ]
    }
   ],
   "source": [
    "# Example of finding similar words\n",
    "similar_words = get_similar_words(\"estimation\", word_embeddings, word2id, id2word)\n",
    "print(\"Words similar to 'estimation':\", similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
