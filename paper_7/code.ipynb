{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31533517",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8b61f",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f72d76",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "224f9918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0db6abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Positional Encoding\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        return config\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angles = np.arange(position)[:, np.newaxis] / np.power(10000, (2*(np.arange(d_model)[np.newaxis,:]//2))/ np.float32(d_model)) \n",
    "        pos_encoding[:, 0::2]=np.sin(angles[:,0::2])\n",
    "        pos_encoding[:, 0::2] = np.cos(angles[:,1::2])\n",
    "        pos_encoding = pos_encoding[np.newxaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    \n",
    "    class MultiHeadAttention(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads):\n",
    "            super(MultiHeadAttention, self).__init__()\n",
    "            assert d_model % num_heads == 0\n",
    "            self.num_heads = num_heads\n",
    "            self.d_model = d_model\n",
    "            self.depth = d_model // num_heads\n",
    "            \n",
    "            self.wq = layers.Dense(d_model)\n",
    "            self.wk = layers.Dense(d_model)\n",
    "            self.wv = layers.Dense(d_model)\n",
    "            \n",
    "            self.dense - layers.Dense(d_model)\n",
    "            \n",
    "            \n",
    "        def get_config(self):\n",
    "            config = super(MultiHeadAttention, self).get_config()\n",
    "            return config\n",
    "        \n",
    "        def split_heads(self, x, batch_size):\n",
    "            x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "            return tf.transpose(x, perm=[0,2,1,3])\n",
    "        def call(self, v, k, q, mask=None):\n",
    "            batch_size = tf.shape(q)[0]\n",
    "            \n",
    "            q = self.wq(q)\n",
    "            k = self.wk(k)\n",
    "            v = self.wv(v)\n",
    "            \n",
    "            q = self.split_heads(q, batch_size)\n",
    "            k = self.split_heads(k, batch_size)\n",
    "            v = self.split_heads(v, batch_size)\n",
    "            \n",
    "            \n",
    "            matmul_qk = tf.matmul(q,k,transpose_b=True)\n",
    "            dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "            scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "            if mask is not None:\n",
    "                mask = tf.expand_dim(mask, axis=1)\n",
    "                scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "            attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "            output = tf.matmul(attention_weights, v)\n",
    "\n",
    "\n",
    "            output = tf.transpose(output, perm=[0,2,1,3])\n",
    "            concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "\n",
    "            return self.dense(concat_attention)\n",
    "    \n",
    "    def feed_forward_network(d_model, d_ff):\n",
    "        return tf.keras.Sequential([\n",
    "            layers.Dense(d_ff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        \n",
    "    class TransformerEncoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, d__ff, dropout_rate=0.1):\n",
    "            super(TransformerEncoderLayer, self).__init__()\n",
    "            \n",
    "            self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "            self.ffn = feed_forward_network(d_model, d_ff)\n",
    "            \n",
    "            self.layernorma1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorma2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            \n",
    "            self.dropout1 = layers.Dropout(dropout_rate)\n",
    "            self.dropout2 = lauers.Dropout(dropout_rate)\n",
    "            \n",
    "        def call(self, x, mask=None, training=False):\n",
    "            attn_output = self.mha(x, x, x, mask)\n",
    "            att_output = self.dropout1(attn_output, training=raining)\n",
    "            out1 = self.layernorm1(x + attn_output)\n",
    "            \n",
    "            ffn_output = self.ffn(out1)\n",
    "            ffn_output = self.dropout2(ffn_output, training=training)\n",
    "            return self.layernorma2(out1 + ffn_output)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8287d4",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84321073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = feed_forward_network(d_model, d_ff)\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        \n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        \n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask = None, training = False):\n",
    "        attn1, attn_weights_block1 = self.mha1(x,x,x, look_ahead_mask)\n",
    "        attn2 = self.dropout1(attn1, training=training)\n",
    "        out = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layer\n",
    "        \n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82eb34",
   "metadata": {},
   "source": [
    "### MLM and NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04f79fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTWithMLMNSP(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(BERTWithMLMNSP, self).__init__()\n",
    "        \n",
    "        self.token_embeddings = layers.Embedding(vocab_size, d_model)\n",
    "        self.position_encoding = PositionalEncoding(max_len, d_model)\n",
    "        self.segment_embeddding = layers.Embedding(2, d_model)\n",
    "        \n",
    "        self.encoder_layers = [TransformerEncoderLayer(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.mlm_layer = layers.Dense(vocab_size)\n",
    "        self.nsp_layer = layers.Dense(2)\n",
    "        \n",
    "    def call(self, input_ids, segment_ids, mask=None, training=False):\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        x = self.token_embeddings(input_ids) + self.segment_embeddings(segment_ids)\n",
    "        x = self.position_encoding(x)\n",
    "        x = self.dropout(x, taining=training)\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask=mask, training=training)\n",
    "            \n",
    "        mlm_logits = self.mlm_layer(x)\n",
    "        \n",
    "        cls_representation = x[:, 0, :]\n",
    "        nsp_logits = self.nsp_layer(cls_reprentation)\n",
    "        return mlm_logits, nsp_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e78c38",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5979ccd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'pos_encoding' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((batch_size, max_len), minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39mvocab_size, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      6\u001b[0m segment_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform((batch_size, max_len), minval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m----> 8\u001b[0m bert_mlm_nsp \u001b[38;5;241m=\u001b[39m \u001b[43mBERTWithMLMNSP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3072\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m mlm_logits, nsp_logits \u001b[38;5;241m=\u001b[39m bert_mlm_nsp(input_ids\u001b[38;5;241m=\u001b[39minput_ids, segment_ids\u001b[38;5;241m=\u001b[39msegment_ids, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLM logits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mlm_logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m, in \u001b[0;36mBERTWithMLMNSP.__init__\u001b[0;34m(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_len, dropout_rate)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(BERTWithMLMNSP, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embeddings \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, d_model)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mPositionalEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_embeddding \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mEmbedding(\u001b[38;5;241m2\u001b[39m, d_model)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers \u001b[38;5;241m=\u001b[39m [TransformerEncoderLayer(d_model, num_heads, d_ff, dropout_rate) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers)]\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[0;34m(self, max_len, d_model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_len, d_model):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28msuper\u001b[39m(PositionalEncoding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mPositionalEncoding.positional_encoding\u001b[0;34m(self, position, d_model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpositional_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m, position, d_model):\n\u001b[1;32m     13\u001b[0m     angles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(position)[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(\u001b[38;5;241m10000\u001b[39m, (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(d_model)[np\u001b[38;5;241m.\u001b[39mnewaxis,:]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(d_model)) \n\u001b[0;32m---> 14\u001b[0m     \u001b[43mpos_encoding\u001b[49m[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(angles[:,\u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     15\u001b[0m     pos_encoding[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(angles[:,\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     16\u001b[0m     pos_encoding \u001b[38;5;241m=\u001b[39m pos_encoding[np\u001b[38;5;241m.\u001b[39mnewxaxis, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'pos_encoding' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "vocab_size = 30522\n",
    "max_len = 512\n",
    "batch_size = 2\n",
    "\n",
    "input_ids = tf.random.uniform((batch_size, max_len), minval=0, maxval=vocab_size, dtype=tf.int32)\n",
    "segment_ids = tf.random.uniform((batch_size, max_len), minval=0, maxval=2, dtype=tf.int32)\n",
    "\n",
    "bert_mlm_nsp = BERTWithMLMNSP(num_layers=12, d_model=768, num_heads=12, d_ff=3072, vocab_size=vocab_size, max_len=max_len)\n",
    "\n",
    "\n",
    "mlm_logits, nsp_logits = bert_mlm_nsp(input_ids=input_ids, segment_ids=segment_ids, training=True)\n",
    "\n",
    "\n",
    "print(\"MLM logits shape:\", mlm_logits.shape)\n",
    "print(\"NSP logits shape:\", nsp_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e27c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
